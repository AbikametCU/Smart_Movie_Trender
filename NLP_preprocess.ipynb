{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An NLP Sandbox\n",
    "\n",
    "Building from The Twitter Political Corpus: https://www.usna.edu/Users/cs/nchamber/data/twitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#YOU have to uncomment this to download punkt!\n",
    "\n",
    "#nltk.download('punkt')  !\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#english_stopwords = stopwords.words('english')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#Possible way to remove common words? https://www.ourren.com/en/2015/01/15/remove-common-words-using-nltk/\n",
    "import numpy as np\n",
    "import csv\n",
    "import sklearn\n",
    "\n",
    "\n",
    "#tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #Removes punctuation\n",
    "\n",
    "a = np.array(list(csv.reader(open('./general-tweets.txt', 'r', encoding = 'utf8'), delimiter='\\t')))\n",
    "b = np.array(list(csv.reader(open('./keyword-tweets.txt', 'r', encoding = 'utf8'), delimiter='\\t')))\n",
    "\n",
    "dataset = np.concatenate((a,b),axis=0)\n",
    "\n",
    "\n",
    "#train_data, test_data = sklearn.model_selection.train_test_split(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " ['ieroween' 'story' 'ieroween' ... 0 0 0]\n",
      " ['trick' 'treating' 'mall' ... 0 0 0]\n",
      " ...\n",
      " ['yea' 'fone' 'iz' ... 0 0 0]\n",
      " ['camerongarcia' 'oh' 'yes' ... 0 0 0]\n",
      " ['rt' 'redstate' 'voter' ... 0 0 0]]\n",
      "Data Saved to file\n"
     ]
    }
   ],
   "source": [
    "f,g= a.shape #(2000, 2)\n",
    "\n",
    "sentence=a[:,1]\n",
    "wordbank=np.zeros((2000, 100), dtype = 'object')   #Guessing that there are 100 words max in each sentence\n",
    "#print(sentence)\n",
    "\n",
    "templist=[]\n",
    "index=0\n",
    "for i in range (1,2000):\n",
    "    index+=1\n",
    "    templist=[]\n",
    "    #print(\"i=\",i)\n",
    "    #print(sentence[i])\n",
    "#https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    sentence[i]= re.sub('[^a-zA-Z]', ' ', sentence[i])\n",
    "    sentence[i]=sentence[i].lower()  #Make all words lowercase\n",
    "    sentence[i]=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence[i])   #remove tags\n",
    "    sentence[i]=re.sub(\"(\\\\d|\\\\W)+\",\" \",sentence[i]) # remove special characters and digits\n",
    "\n",
    "    \n",
    "    #print(\"Cleaned=\", sentence[i])\n",
    "    #sentence[i]_token= tokenizer.tokenize(sentence[i])\n",
    "\n",
    "    templist= sentence[i].split()\n",
    "    #print(sentence[i])\n",
    "\n",
    "#for i in range(len(sentence[i]_token)):\n",
    "  #  sentence[i]_token[i]=sentence[i]_token[i].lower()  #Make all words lowercase\n",
    "    \n",
    "    #print(templist)\n",
    "#print(lem.lemmatize(sentence[i]_token))\n",
    "##Stemming\n",
    "    ps=PorterStemmer() #Stemming   \n",
    "    lem = WordNetLemmatizer()#Lemmatisation\n",
    "    templist = [lem.lemmatize(word) for word in templist if not word in  \n",
    "                stop_words] \n",
    "    #templist = \" \".join(templist)\n",
    "    #print(\"TEMPLIST= \", templist)\n",
    "    for g in range(len(templist)):\n",
    "            wordbank[index,g]=templist[g]\n",
    "print(wordbank)\n",
    "numpy_data = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "df = pd.DataFrame(wordbank)\n",
    "\n",
    "  \n",
    "        # saving the dataframe \n",
    "df.to_csv('Wordbank1.csv') \n",
    "print(\"Data Saved to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
